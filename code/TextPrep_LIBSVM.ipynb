{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load textprep.py\n",
    "import re\n",
    "import string\n",
    "import os\n",
    "import psutil\n",
    "import gc\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "from scipy.sparse import hstack\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.datasets import dump_svmlight_file\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading data\n",
      "nlp train done\n",
      "nlp test done\n",
      "Creating numerical features\n",
      "Tfidf on word\n",
      "Tfidf on char n_gram\n",
      "1362\n",
      "Stacking matrices\n"
     ]
    }
   ],
   "source": [
    "\n",
    "cont_patterns = [\n",
    "        (b'US', b'United States'),\n",
    "        (b'IT', b'Information Technology'),\n",
    "        (b'(W|w)on\\'t', b'will not'),\n",
    "        (b'(C|c)an\\'t', b'can not'),\n",
    "        (b'(I|i)\\'m', b'i am'),\n",
    "        (b'(A|a)in\\'t', b'is not'),\n",
    "        (b'(\\w+)\\'ll', b'\\g<1> will'),\n",
    "        (b'(\\w+)n\\'t', b'\\g<1> not'),\n",
    "        (b'(\\w+)\\'ve', b'\\g<1> have'),\n",
    "        (b'(\\w+)\\'s', b'\\g<1> is'),\n",
    "        (b'(\\w+)\\'re', b'\\g<1> are'),\n",
    "        (b'(\\w+)\\'d', b'\\g<1> would')\n",
    "    ]\n",
    "patterns = [(re.compile(regex), repl) for (regex, repl) in cont_patterns]\n",
    "\n",
    "def prepare_for_char_n_gram(text):\n",
    "  clean = bytes(text.lower(), encoding=\"utf-8\") #lower case\n",
    "  clean = clean.replace(b\"\\n\", b\" \")   # spaces and tabs\n",
    "  clean = clean.replace(b\"\\t\", b\" \")\n",
    "  clean = clean.replace(b\"\\b\", b\" \")\n",
    "  clean = clean.replace(b\"\\r\", b\" \")\n",
    "  for (pattern, repl) in patterns:\n",
    "      clean = re.sub(pattern, repl, clean)  # contractions?\n",
    "  exclude = re.compile(b'[%s]' % re.escape(bytes(string.punctuation, encoding='utf-8')))\n",
    "  clean = b\" \".join([exclude.sub(b'', token) for token in clean.split()])  # punctuation\n",
    "  clean = re.sub(b\"\\d+\", b\" \", clean)  #numbers\n",
    "  clean = re.sub(b'\\s+', b' ', clean)   # spaces\n",
    "  clean = re.sub(b\" \", b\"# #\", clean)   # add # signs?\n",
    "  clean = re.sub(b'\\s+$', b'', clean) # ending spaces\n",
    "  return str(clean, 'utf-8')\n",
    "   \n",
    "def count_regexp_occ(regexp=\"\", text=None):\n",
    "    return len(re.findall(regexp, text))\n",
    "\n",
    "def perform_nlp(df):     # Check all sorts of content\n",
    "    df[\"ant_slash_n\"] = df[\"comment_text\"].apply(lambda x: count_regexp_occ(r\"\\n\", x))\n",
    "    df[\"raw_word_len\"] = df[\"comment_text\"].apply(lambda x: len(x.split()))\n",
    "    df[\"raw_char_len\"] = df[\"comment_text\"].apply(lambda x: len(x))\n",
    "    df[\"nb_upper\"] = df[\"comment_text\"].apply(lambda x: count_regexp_occ(r\"[A-Z]\", x))\n",
    "    df[\"nb_fk\"] = df[\"comment_text\"].apply(lambda x: count_regexp_occ(r\"[Ff]\\S{2}[Kk]\", x))\n",
    "    df[\"nb_sk\"] = df[\"comment_text\"].apply(lambda x: count_regexp_occ(r\"[Ss]\\S{2}[Kk]\", x))\n",
    "    df[\"nb_dk\"] = df[\"comment_text\"].apply(lambda x: count_regexp_occ(r\"[dD]ick\", x))\n",
    "    df[\"nb_you\"] = df[\"comment_text\"].apply(lambda x: count_regexp_occ(r\"\\W[Yy]ou\\W\", x))\n",
    "    df[\"nb_mother\"] = df[\"comment_text\"].apply(lambda x: count_regexp_occ(r\"\\Wmother\\W\", x))\n",
    "    df[\"nb_nigger\"] = df[\"comment_text\"].apply(lambda x: count_regexp_occ(r\"\\Wnigger\\W\", x))\n",
    "    df[\"start_with_columns\"] = df[\"comment_text\"].apply(lambda x: count_regexp_occ(r\"^\\:+\", x))\n",
    "    df[\"has_timestamp\"] = df[\"comment_text\"].apply(lambda x: count_regexp_occ\n",
    "        (r\"\\d{2}|:\\d{2}\", x))\n",
    "    df[\"has_date_long\"] = df[\"comment_text\"].apply(lambda x: count_regexp_occ\n",
    "    (r\"\\D\\d{2}:\\d{2},\\d{1,2} \\w+ \\d{4}\", x))\n",
    "    df[\"has_date_short\"] = df[\"comment_text\"].apply(lambda x: count_regexp_occ\n",
    "        (r\"\\D\\d{1,2} \\w+ \\d{4}\", x))\n",
    "    df[\"has_http\"] = df[\"comment_text\"].apply(lambda x: count_regexp_occ\n",
    "        (r\"http[s]{0,1}://\\S+\", x))\n",
    "    df[\"has_mail\"] = df[\"comment_text\"].apply(lambda x: count_regexp_occ(r\"\\S+\\@\\w+\\.\\w+\", x))\n",
    "    df[\"has_emphasize_equal\"] = df[\"comment_text\"].apply(lambda x: count_regexp_occ\n",
    "        (r\"\\={2}.+\\={2}\", x))\n",
    "    df[\"has_emphasize_quotes\"] = df[\"comment_text\"].apply(lambda x: count_regexp_occ\n",
    "        (r\"\\\"{4}\\S+\\\"{4}\", x))\n",
    "    \n",
    "    ip_regexp = r\"\"\"(([0-9]|[1-9][0-9]|1[0-9]{2}|2[0-4][0-9]|25[0-5])\\.)\n",
    "        {3}([0-9]|[1-9][0-9]|1[0-9]{2}|2[0-4][0-9]|25[0-5])$\"\"\"\n",
    "    # Now clean comments\n",
    "    df[\"clean_comment\"] = df[\"comment_text\"].apply(lambda x: prepare_for_char_n_gram(x))\n",
    "\n",
    "    df[\"clean_chars\"] = df[\"clean_comment\"].apply(lambda x: len(set(x)))\n",
    "    df[\"clean_chars_ratio\"] = df[\"clean_comment\"].apply(lambda x: len(set(x))) /            df[\"clean_comment\"].apply(lambda x: 1 + min(99, len(x)))\n",
    "\n",
    "    # Get the exact length in words and characters\n",
    "    df[\"clean_word_len\"] = df[\"clean_comment\"].apply(lambda x: len(x.split()))\n",
    "    df[\"clean_char_len\"] = df[\"clean_comment\"].apply(lambda x: len(x))\n",
    "\n",
    "\n",
    "def char_analyzer(text):\n",
    "  tokens = text.split()\n",
    "  return [token[i: i+3] for token in tokens for i in range(len(token) - 2)]\n",
    "\n",
    "gc.enable()\n",
    "\n",
    "print('reading data')\n",
    "train = pd.read_csv('../input/train.csv').fillna(' ')\n",
    "test = pd.read_csv('../input/test.csv').fillna(' ')\n",
    "\n",
    "class_names = list(train)[-6:]\n",
    "\n",
    "\n",
    "perform_nlp(train)\n",
    "print('nlp train done')\n",
    "perform_nlp(test)\n",
    "print('nlp test done')\n",
    "\n",
    "train_text = train['clean_comment']\n",
    "test_text = test['clean_comment']\n",
    "all_text = pd.concat([train_text, test_text])\n",
    "\n",
    "\n",
    "print(\"Creating numerical features\")\n",
    "num_features = [f_ for f_ in train.columns\n",
    "                if f_ not in [\"comment_text\", \"clean_comment\", \"id\", \"remaining_chars\", \n",
    "                    'has_ip_address'] + class_names]\n",
    "\n",
    "\n",
    "skl = MinMaxScaler()\n",
    "\n",
    "train_num_features =(skl.fit_transform(train[num_features]))   #csr_matrix\n",
    "test_num_features = (skl.fit_transform(test[num_features]))\n",
    "\n",
    "print(\"Tfidf on word\")\n",
    "word_vectorizer = TfidfVectorizer(\n",
    "    sublinear_tf=True,\n",
    "    strip_accents='unicode',\n",
    "    analyzer='word',\n",
    "    token_pattern=r'\\w{1,}',\n",
    "    stop_words='english',\n",
    "    ngram_range=(1, 2),\n",
    "    max_features=50000)                 #20000\n",
    "word_vectorizer.fit(all_text)\n",
    "train_word_features = word_vectorizer.transform(train_text)\n",
    "test_word_features = word_vectorizer.transform(test_text)\n",
    "del word_vectorizer\n",
    "gc.collect()\n",
    "\n",
    "\n",
    "print(\"Tfidf on char n_gram\")\n",
    "char_vectorizer = TfidfVectorizer(\n",
    "    sublinear_tf=True,\n",
    "    strip_accents='unicode',\n",
    "    tokenizer=char_analyzer,\n",
    "    analyzer='word',\n",
    "    ngram_range=(1, 1),\n",
    "    max_features=80000)                    #50000\n",
    "char_vectorizer.fit(all_text)\n",
    "train_char_features = char_vectorizer.transform(train_text)\n",
    "test_char_features = char_vectorizer.transform(test_text)\n",
    "    \n",
    "print((train_char_features>0).sum(axis=1).max())\n",
    "\n",
    "\n",
    "gc.collect()\n",
    "\n",
    "\n",
    "print(\"Stacking matrices\")\n",
    "csr_trn = hstack(\n",
    "    [train_char_features, \n",
    "    train_word_features, \n",
    "    train_num_features],\n",
    "  format='csr') #.tocsr()\n",
    "\n",
    "csr_sub = hstack([test_char_features, \n",
    "        test_word_features, \n",
    "        test_num_features], \n",
    "  format='csr')\n",
    "\n",
    "\n",
    "# del train_text\n",
    "# del test_text\n",
    "# del train_word_features\n",
    "# del test_word_features\n",
    "# del train_char_features\n",
    "# del test_char_features\n",
    "# del train_num_features\n",
    "# del test_num_features\n",
    "# gc.collect()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,c in enumerate(class_names):\n",
    "    y = train[class_names[i]]\n",
    "    fname = \"../input/trainsparse{}.txt\".format(c)\n",
    "    dump_svmlight_file(csr_trn, y, fname, zero_based=True, comment=None, query_id=None, multilabel=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_fake = np.zeros(test.shape[0])\n",
    "dump_svmlight_file(csr_sub, y_fake, '../input/testsparse.txt', zero_based=True, comment=None, query_id=None, multilabel=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py35]",
   "language": "python",
   "name": "conda-env-py35-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "419px",
    "left": "378px",
    "right": "20px",
    "top": "120px",
    "width": "350px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
